# Advanced Multilayer Perceptrons and Keras  

## Understand Model Behavior During Training By Plotting History
* history = model.fit()的返回object包含了一个叫做history的字典，里头包含了每一次epoch的详细记录(loss与用户需求的metric)
* history.history['xxx']
#
	# Fit the model
	history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
	# list all data in history
	print(history.history.keys())
	# summarize history for accuracy
	plt.plot(history.history['accuracy'])
	plt.plot(history.history['val_accuracy'])
	plt.title('model accuracy')
	plt.ylabel('accuracy')
	plt.xlabel('epoch')
	plt.legend(['train', 'test'], loc='upper left')
	plt.show()

## Reduce Overfitting With Dropout Regularization
* Drop out rate is about **0.2~0.5**. 0.2 is a good starting point.
* Use **larger network** to make the model robust against dropout and thus learn indepdendet representations.
* Dropout can be used in **input and hidden layers**.
* After dropout, you may want to **increase learning rate** by a factor of 10 to 100 and use **high momentum** value of 0.9 or 0.99.
* **Constrain the size of network weights**:  
	* kernel_constraint=maxnorm(3): maxnorm(m) will, if the L2-Norm of your weights exceeds m, scale your whole weight matrix by a factor that reduces the norm to m
#
	
	


	def create_model():
		# create model
		model = Sequential()
		model.add(Dropout(0.2, input_shape=(60,)))
		model.add(Dense(60, input_dim=60, activation='relu', kernel_constraint=maxnorm(3)))
		model.add(Dropout(0.2))
		model.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))
		model.add(Dropout(0.2))
		model.add(Dense(1, activation='sigmoid'))
		# Compile model
		sgd = SGD(lr=0.1, momentum=0.9)
		model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
	return model


## Lift Performance With Learning Rate Schedules
![Page0](https://github.com/zhukuixi/AshenOne/blob/master/JasonBookSeries/DeepLearningWithPython/image/LearningRateDecayWithTime.png.png)  